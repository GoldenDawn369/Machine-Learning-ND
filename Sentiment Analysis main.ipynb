{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "905601c1",
   "metadata": {},
   "source": [
    "# FinBert neutral positive bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48d9e800",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 4, Predictions: tensor([1, 1, 1, 1])\n",
      "Batch size: 1, Predictions: tensor([1])\n",
      "Batch size: 4, Predictions: tensor([1, 1, 1, 1])\n",
      "Batch size: 4, Predictions: tensor([1, 1, 1, 1])\n",
      "Batch size: 4, Predictions: tensor([0, 2, 2, 1])\n",
      "Batch size: 4, Predictions: tensor([1, 2, 2, 2])\n",
      "Batch size: 2, Predictions: tensor([2, 2])\n",
      "                                                File Predicted Sentiment\n",
      "0  Bitcoin_Extends_Pullback;_Support_at_$37K,_Res...             neutral\n",
      "1  Retail_Interest_in_Bitcoin_Is_Dwindling,_Googl...             neutral\n",
      "2  Seized_Silk_Road_Bitcoin_to_Clear_Ross_Ulbrich...            positive\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "# Load the FinBERT model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('ProsusAI/finbert')\n",
    "tokenizer = BertTokenizer.from_pretrained('ProsusAI/finbert')\n",
    "\n",
    "# Specify the directory containing the text files\n",
    "directory = r\"C:\\Users\\Rober\\Personal Projects\\STOCK MONEY\\Scrapers\\Data\\2022\\April\\22\"\n",
    "\n",
    "def load_data(directory):\n",
    "    files = [f for f in os.listdir(directory) if f.endswith('.txt')]\n",
    "    texts = []\n",
    "    for file in files:\n",
    "        with open(os.path.join(directory, file), 'r', encoding='utf-8') as f:\n",
    "            texts.append((f.read(), file))\n",
    "    return texts, files\n",
    "\n",
    "def segment_text(text, max_length=512, overlap=50):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    segments = []\n",
    "    segment = \"\"\n",
    "    for sentence in sentences:\n",
    "        if len(tokenizer.tokenize(segment + sentence)) <= max_length:\n",
    "            segment += \" \" + sentence\n",
    "        else:\n",
    "            segments.append(segment)\n",
    "            segment = sentence[max(0, len(sentence) - overlap):]\n",
    "    if segment:\n",
    "        segments.append(segment)\n",
    "    return segments\n",
    "\n",
    "def process_segments(segments):\n",
    "    # Tokenize all segments in a single batch, with padding enabled\n",
    "    inputs = tokenizer(\n",
    "        segments,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    return inputs['input_ids'], inputs['attention_mask']\n",
    "\n",
    "\n",
    "def get_predictions(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    for batch in dataloader:\n",
    "        input_ids, attention_mask = [t.to(device) for t in batch]  # Move data to the device\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        print(f\"Batch size: {len(input_ids)}, Predictions: {preds}\")\n",
    "        predictions.extend(preds.cpu())  # Move predictions back to CPU for further processing\n",
    "    return predictions\n",
    "\n",
    "texts, files = load_data(directory)\n",
    "# Initialize empty lists to hold file names and predicted labels\n",
    "all_files = []\n",
    "all_predicted_labels = []\n",
    "\n",
    "# Process each file, generate predictions, and gather the results\n",
    "for text, file in zip(texts, files):\n",
    "    segments = segment_text(text[0])  # text is a tuple of (text_content, file_name)\n",
    "    input_ids_tensor, attention_masks_tensor = process_segments(segments)\n",
    "    dataset = TensorDataset(input_ids_tensor, attention_masks_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=4)\n",
    "    predictions = get_predictions(model, dataloader)\n",
    "    # Get the most common prediction for each file\n",
    "    most_common_prediction = max(set(predictions), key=predictions.count)\n",
    "    # Update the lists with the file name and the most common prediction\n",
    "    all_files.append(file)\n",
    "    all_predicted_labels.append(label_map[most_common_prediction.item()])\n",
    "\n",
    "# Create a DataFrame to hold the results\n",
    "results_df = pd.DataFrame({'File': all_files, 'Predicted Sentiment': all_predicted_labels})\n",
    "\n",
    "# Optionally, save the results to a CSV file\n",
    "results_df.to_csv('classification_results.csv', index=False)\n",
    "\n",
    "# Print the results\n",
    "print(results_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ccda9e",
   "metadata": {},
   "source": [
    "# Positive Negative CryptoBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a3cb0b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5acd4355e6c4cf1b0627c171f2f5823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f81bfd2060514e5b8f19a04d14486aae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 4, Predictions: tensor([1, 1, 1, 1], device='cuda:0')\n",
      "Batch size: 1, Predictions: tensor([1], device='cuda:0')\n",
      "Batch size: 4, Predictions: tensor([2, 1, 1, 2], device='cuda:0')\n",
      "Batch size: 4, Predictions: tensor([2, 2, 1, 1], device='cuda:0')\n",
      "Batch size: 3, Predictions: tensor([2, 1, 1], device='cuda:0')\n",
      "Batch size: 4, Predictions: tensor([1, 1, 1, 1], device='cuda:0')\n",
      "Batch size: 1, Predictions: tensor([1], device='cuda:0')\n",
      "                                                File Predicted Sentiment\n",
      "0  Bitcoin_Extends_Pullback;_Support_at_$37K,_Res...            Positive\n",
      "1  Retail_Interest_in_Bitcoin_Is_Dwindling,_Googl...            Positive\n",
      "2  Seized_Silk_Road_Bitcoin_to_Clear_Ross_Ulbrich...            Positive\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "\n",
    "# Load the CryptoBERT model and tokenizer\n",
    "model = RobertaForSequenceClassification.from_pretrained('ElKulako/cryptobert')\n",
    "tokenizer = RobertaTokenizer.from_pretrained('ElKulako/cryptobert')\n",
    "\n",
    "# Specify the directory containing the text files\n",
    "directory = r\"C:\\Users\\Rober\\Personal Projects\\STOCK MONEY\\Scrapers\\Data\\2022\\April\\22\"\n",
    "\n",
    "def load_data(directory):\n",
    "    files = [f for f in os.listdir(directory) if f.endswith('.txt')]\n",
    "    texts = []\n",
    "    for file in files:\n",
    "        with open(os.path.join(directory, file), 'r', encoding='utf-8') as f:\n",
    "            texts.append((f.read(), file))\n",
    "    return texts, files\n",
    "\n",
    "def segment_text(text, max_length=512, overlap=50):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    segments = []\n",
    "    segment = \"\"\n",
    "    for sentence in sentences:\n",
    "        if len(tokenizer.tokenize(segment + sentence)) <= max_length:\n",
    "            segment += \" \" + sentence\n",
    "        else:\n",
    "            segments.append(segment)\n",
    "            segment = sentence[max(0, len(sentence) - overlap):]\n",
    "    if segment:\n",
    "        segments.append(segment)\n",
    "    return segments\n",
    "\n",
    "def process_segments(segments):\n",
    "    # Tokenize all segments in a single batch, with padding enabled\n",
    "    inputs = tokenizer(\n",
    "        segments,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    return inputs['input_ids'], inputs['attention_mask']\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_predictions(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    for batch in dataloader:\n",
    "        input_ids, attention_mask = [t.to(device) for t in batch]  # Move data to the device\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        print(f\"Batch size: {len(input_ids)}, Predictions: {preds}\")\n",
    "        predictions.extend(preds.cpu())  # Move predictions back to CPU for further processing\n",
    "    return predictions\n",
    "\n",
    "texts, files = load_data(directory)\n",
    "# Initialize empty lists to hold file names and predicted labels\n",
    "all_files = []\n",
    "all_predicted_labels = []\n",
    "\n",
    "# Define label_map to map numerical predictions to sentiment labels\n",
    "label_map = {0: 'Negative', 1: 'Positive'}\n",
    "\n",
    "# Process each file, generate predictions, and gather the results\n",
    "for text, file in zip(texts, files):\n",
    "    segments = segment_text(text[0])  # text is a tuple of (text_content, file_name)\n",
    "    input_ids_tensor, attention_masks_tensor = process_segments(segments)\n",
    "    dataset = TensorDataset(input_ids_tensor, attention_masks_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=4)\n",
    "    predictions = get_predictions(model, dataloader)\n",
    "    # Get the most common prediction for each file\n",
    "    most_common_prediction = max(set(predictions), key=predictions.count)\n",
    "    # Update the lists with the file name and the most common prediction\n",
    "    all_files.append(file)\n",
    "    all_predicted_labels.append(label_map[most_common_prediction.item()])\n",
    "\n",
    "# Create a DataFrame to hold the results\n",
    "results_df = pd.DataFrame({'File': all_files, 'Predicted Sentiment': all_predicted_labels})\n",
    "\n",
    "# Optionally, save the results to a CSV file\n",
    "results_df.to_csv('classification_results.csv', index=False)\n",
    "\n",
    "# Print the results\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b63a21e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db906d1dd6bc40d5a6c85118391942be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/725 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ecebcba2df34fa3adc12899c8fc3b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at j-hartmann/sentiment-roberta-large-english-3-classes were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa165e1c55d64b688f0fe853b0867eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6190113d95da4c43b0a455bad19cb8e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e53720c6cb184cfeb0dc2a6b59642903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5901b9eec72484ebdb3c83a6100bcea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/1.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                File Predicted Sentiment\n",
      "0  Bitcoin_Extends_Pullback;_Support_at_$37K,_Res...             neutral\n",
      "1  Retail_Interest_in_Bitcoin_Is_Dwindling,_Googl...             neutral\n",
      "2  Seized_Silk_Road_Bitcoin_to_Clear_Ross_Ulbrich...             neutral\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "\n",
    "# Load the RoBERTa model and tokenizer\n",
    "model = RobertaForSequenceClassification.from_pretrained('j-hartmann/sentiment-roberta-large-english-3-classes')\n",
    "tokenizer = RobertaTokenizer.from_pretrained('j-hartmann/sentiment-roberta-large-english-3-classes')\n",
    "\n",
    "# Specify the directory containing the text files\n",
    "directory = r\"C:\\Users\\Rober\\Personal Projects\\STOCK MONEY\\Scrapers\\Data\\2022\\April\\22\"\n",
    "\n",
    "def load_data(directory):\n",
    "    files = [f for f in os.listdir(directory) if f.endswith('.txt')]\n",
    "    texts = []\n",
    "    for file in files:\n",
    "        with open(os.path.join(directory, file), 'r', encoding='utf-8') as f:\n",
    "            texts.append((f.read(), file))\n",
    "    return texts, files\n",
    "\n",
    "def segment_text(text, max_length=512, overlap=50):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    segments = []\n",
    "    segment = \"\"\n",
    "    for sentence in sentences:\n",
    "        if len(tokenizer.tokenize(segment + sentence)) <= max_length:\n",
    "            segment += \" \" + sentence\n",
    "        else:\n",
    "            segments.append(segment)\n",
    "            segment = sentence[max(0, len(sentence) - overlap):]\n",
    "    if segment:\n",
    "        segments.append(segment)\n",
    "    return segments\n",
    "\n",
    "def process_segments(segments):\n",
    "    # Tokenize all segments in a single batch, with padding enabled\n",
    "    inputs = tokenizer(\n",
    "        segments,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    return inputs['input_ids'], inputs['attention_mask']\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_predictions(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    for batch in dataloader:\n",
    "        input_ids, attention_mask = [t.to(device) for t in batch]  # Move data to the device\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        predictions.extend(preds.cpu())  # Move predictions back to CPU for further processing\n",
    "    return predictions\n",
    "\n",
    "texts, files = load_data(directory)\n",
    "# Initialize empty lists to hold file names and predicted labels\n",
    "all_files = []\n",
    "all_predicted_labels = []\n",
    "\n",
    "# Process each file, generate predictions, and gather the results\n",
    "for text, file in zip(texts, files):\n",
    "    segments = segment_text(text[0])  # text is a tuple of (text_content, file_name)\n",
    "    input_ids_tensor, attention_masks_tensor = process_segments(segments)\n",
    "    dataset = TensorDataset(input_ids_tensor, attention_masks_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=4)\n",
    "    predictions = get_predictions(model, dataloader)\n",
    "    # Get the most common prediction for each file\n",
    "    label_map = {0: 'negative', 1: 'neutral', 2: 'positive'}  # Define the mapping between label IDs and label names\n",
    "    most_common_prediction = max(set(predictions), key=predictions.count)\n",
    "    # Update the lists with the file name and the most common prediction\n",
    "    all_files.append(file)\n",
    "    all_predicted_labels.append(label_map[most_common_prediction.item()])\n",
    "\n",
    "# Create a DataFrame to hold the results\n",
    "results_df = pd.DataFrame({'File': all_files, 'Predicted Sentiment': all_predicted_labels})\n",
    "\n",
    "# Optionally, save the results to a CSV file\n",
    "results_df.to_csv('classification_results.csv', index=False)\n",
    "\n",
    "# Print the results\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47448f1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2712906331.py, line 106)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Rober\\AppData\\Local\\Temp\\ipykernel_22324\\2712906331.py\"\u001b[1;36m, line \u001b[1;32m106\u001b[0m\n\u001b[1;33m    print(f\"Counts for {day}: {count}\")l\u001b[0m\n\u001b[1;37m                                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "# Load the FinBERT model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('ProsusAI/finbert')\n",
    "tokenizer = BertTokenizer.from_pretrained('ProsusAI/finbert')\n",
    "\n",
    "# Specify the directory containing the text files\n",
    "directory = r\"C:\\Users\\Rober\\Personal Projects\\STOCK MONEY\\Scrapers\\Data\\2023\\September\"\n",
    "\n",
    "def load_data(directory):\n",
    "    texts = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        print(dirs)\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                    texts.append((f.read(), os.path.basename(root), file))\n",
    "    return texts\n",
    "\n",
    "\n",
    "def segment_text(text, max_length=512, overlap=50):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    segments = []\n",
    "    segment = \"\"\n",
    "    for sentence in sentences:\n",
    "        if len(tokenizer.tokenize(segment + sentence)) <= max_length:\n",
    "            segment += \" \" + sentence\n",
    "        else:\n",
    "            segments.append(segment)\n",
    "            segment = sentence[max(0, len(sentence) - overlap):]\n",
    "    if segment:\n",
    "        segments.append(segment)\n",
    "    return segments\n",
    "\n",
    "def process_segments(segments):\n",
    "    # Tokenize all segments in a single batch, with padding enabled\n",
    "    inputs = tokenizer(\n",
    "        segments,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    return inputs['input_ids'], inputs['attention_mask']\n",
    "\n",
    "\n",
    "def get_predictions(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    for batch in dataloader:\n",
    "        input_ids, attention_mask = [t.to(device) for t in batch]  # Move data to the device\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        print(f\"Batch size: {len(input_ids)}, Predictions: {preds}\")\n",
    "        predictions.extend(preds.cpu())  # Move predictions back to CPU for further processing\n",
    "    return predictions\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "texts = load_data(directory)\n",
    "\n",
    "# Initialize empty lists to hold file names, predicted labels, and dates\n",
    "all_files = []\n",
    "all_predicted_labels = []\n",
    "all_days = []\n",
    "\n",
    "# Initialize Counters for daily and overall sentiment counts\n",
    "day_counts = {}\n",
    "overall_counts = Counter()\n",
    "\n",
    "label_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "\n",
    "# Process each file, generate predictions, and gather the results\n",
    "for text, day, file in texts:\n",
    "    segments = segment_text(text)\n",
    "    input_ids_tensor, attention_masks_tensor = process_segments(segments)\n",
    "    dataset = TensorDataset(input_ids_tensor, attention_masks_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=4)\n",
    "    predictions = get_predictions(model, dataloader)\n",
    "    prediction_labels = [label_map[pred.item()] for pred in predictions]\n",
    "    most_common_prediction = max(set(predictions), key=predictions.count)\n",
    "    day_counts.setdefault(day, Counter()).update(prediction_labels)\n",
    "    overall_counts.update(prediction_labels)\n",
    "    all_files.append(file)\n",
    "    all_predicted_labels.append(label_map[most_common_prediction.item()])\n",
    "    all_days.append(day)\n",
    "\n",
    "# Create a DataFrame to hold the results\n",
    "results_df = pd.DataFrame({'File': all_files, 'Day': all_days, 'Predicted Sentiment': all_predicted_labels})\n",
    "\n",
    "# Optionally, save the results to a CSV file\n",
    "results_df.to_csv('classification_results.csv', index=False)\n",
    "\n",
    "# Print the results\n",
    "print(results_df)\n",
    "print(f\"Overall counts for the month: {overall_counts}\")\n",
    "for day, count in day_counts.items():\n",
    "    print(f\"Counts for {day}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e1076e",
   "metadata": {},
   "source": [
    "# Average sentiment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c062593d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '10', '12', '13', '14', '15', '16', '19', '2', '20', '21', '22', '23', '26', '27', '28', '29', '30', '5', '6', '7', '8', '9']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "                                                  File Day  \\\n",
      "0    Bitcoin_Lingers_Under_27K_to_Continue_Its_May_...   1   \n",
      "1    First_Mover_Americas_Bitcoin_Begins_June_Dropp...   1   \n",
      "2    First_Mover_Asia_Bitcoin_Settles_Above_271K_Af...   1   \n",
      "3    Litecoin_Starts_June_Strong_as_Investors_Eye_A...   1   \n",
      "4    On_Heels_of_First_Losing_Month_of_2023_Bitcoin...   1   \n",
      "..                                                 ...  ..   \n",
      "190  Bitcoin_Payments_Firm_Strike_Moves_Custody_InH...   9   \n",
      "191  Bitcoin_Trades_at_Narrow_Discount_on_BinanceUS...   9   \n",
      "192  Datos_de_empleo_en_EE_UU_dejaron_una_débil_esp...   9   \n",
      "193  First_Mover_Americas_BinanceUS_Suspends_Dollar...   9   \n",
      "194  First_Mover_Asia_Bitcoin_Remains_Resilient_Nea...   9   \n",
      "\n",
      "     Average Sentiment Score  \n",
      "0                  -0.111263  \n",
      "1                   0.245130  \n",
      "2                   0.273907  \n",
      "3                  -0.371261  \n",
      "4                   0.099406  \n",
      "..                       ...  \n",
      "190                 0.120291  \n",
      "191                 0.103477  \n",
      "192                 0.823904  \n",
      "193                -0.006357  \n",
      "194                 0.136809  \n",
      "\n",
      "[195 rows x 3 columns]\n",
      "Average sentiment score for the month: 0.33888131380081177\n",
      "Average sentiment score for 1: 0.027183884754776955\n",
      "Average sentiment score for 10: -0.25974440574645996\n",
      "Average sentiment score for 12: 0.5901479721069336\n",
      "Average sentiment score for 13: 0.36582157015800476\n",
      "Average sentiment score for 14: 0.43259066343307495\n",
      "Average sentiment score for 15: 0.47368451952934265\n",
      "Average sentiment score for 16: 0.30289772152900696\n",
      "Average sentiment score for 19: 0.33878621459007263\n",
      "Average sentiment score for 2: 0.3778739869594574\n",
      "Average sentiment score for 20: 0.2911684513092041\n",
      "Average sentiment score for 21: 0.3337576985359192\n",
      "Average sentiment score for 22: 0.2985907196998596\n",
      "Average sentiment score for 23: 0.38104182481765747\n",
      "Average sentiment score for 26: 0.2583691477775574\n",
      "Average sentiment score for 27: 0.4613613188266754\n",
      "Average sentiment score for 28: 0.29491376876831055\n",
      "Average sentiment score for 29: 0.04577624797821045\n",
      "Average sentiment score for 30: 0.07900820672512054\n",
      "Average sentiment score for 5: 0.5183222889900208\n",
      "Average sentiment score for 6: 0.4032634496688843\n",
      "Average sentiment score for 7: 0.33094945549964905\n",
      "Average sentiment score for 8: 0.2988564074039459\n",
      "Average sentiment score for 9: 0.23562487959861755\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch.nn.functional as F  # Import the functional module from PyTorch\n",
    "from collections import defaultdict\n",
    "import numpy as np  #h\n",
    "# Load the FinBERT model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('ProsusAI/finbert')\n",
    "tokenizer = BertTokenizer.from_pretrained('ProsusAI/finbert')\n",
    "\n",
    "# Specify the directory containing the text files\n",
    "directory = r\"C:\\Users\\Rober\\Personal Projects\\STOCK MONEY\\Scrapers\\Data\\2023\\June\"\n",
    "\n",
    "def load_data(directory):\n",
    "    texts = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        print(dirs)\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                    texts.append((f.read(), os.path.basename(root), file))\n",
    "    return texts\n",
    "\n",
    "\n",
    "def segment_text(text, max_length=512, overlap=50):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    segments = []\n",
    "    segment = \"\"\n",
    "    for sentence in sentences:\n",
    "        if len(tokenizer.tokenize(segment + sentence)) <= max_length:\n",
    "            segment += \" \" + sentence\n",
    "        else:\n",
    "            segments.append(segment)\n",
    "            segment = sentence[max(0, len(sentence) - overlap):]\n",
    "    if segment:\n",
    "        segments.append(segment)\n",
    "    return segments\n",
    "\n",
    "def process_segments(segments):\n",
    "    # Tokenize all segments in a single batch, with padding enabled\n",
    "    inputs = tokenizer(\n",
    "        segments,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    return inputs['input_ids'], inputs['attention_mask']\n",
    "\n",
    "\n",
    "\n",
    "def get_predictions(model, dataloader):\n",
    "    model.eval()\n",
    "    sentiment_scores = []  # List to hold the sentiment scores\n",
    "    for batch in dataloader:\n",
    "        input_ids, attention_mask = [t.to(device) for t in batch]  # Move data to the device\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probs = F.softmax(logits, dim=1)  # Convert logits to probabilities\n",
    "        # Calculate sentiment score: prob(positive) - prob(negative)\n",
    "        scores = probs[:, 2] - probs[:, 0]\n",
    "        sentiment_scores.extend(scores.cpu())  # Move scores back to CPU for further processing\n",
    "    return sentiment_scores\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "texts = load_data(directory)\n",
    "\n",
    "# Initialize empty lists to hold file names, predicted labels, and dates\n",
    "all_files = []\n",
    "all_predicted_labels = []\n",
    "all_days = []\n",
    "day_scores = defaultdict(list)\n",
    "monthly_scores = []\n",
    "\n",
    "# Initialize Counters for daily and overall sentiment counts\n",
    "day_counts = {}\n",
    "overall_counts = Counter()\n",
    "\n",
    "label_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "\n",
    "# Process each file, generate predictions, and gather the results\n",
    "for text, day, file in texts:\n",
    "    segments = segment_text(text)\n",
    "    input_ids_tensor, attention_masks_tensor = process_segments(segments)\n",
    "    dataset = TensorDataset(input_ids_tensor, attention_masks_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=4)\n",
    "    sentiment_scores = get_predictions(model, dataloader)  # Get the sentiment scores\n",
    "    avg_sentiment_score = np.mean(sentiment_scores)  # Get the average sentiment score\n",
    "    day_scores[day].append(avg_sentiment_score)  # Store the average sentiment score\n",
    "    monthly_scores.append(avg_sentiment_score)\n",
    "    all_files.append(file)\n",
    "    all_days.append(day)\n",
    "\n",
    "# Create a DataFrame to hold the results\n",
    "results_df = pd.DataFrame({'File': all_files, 'Day': all_days, 'Average Sentiment Score': monthly_scores})\n",
    "\n",
    "# Optionally, save the results to a CSV file\n",
    "results_df.to_csv('classification_results_test.csv', index=False)\n",
    "\n",
    "# Print the results\n",
    "print(results_df)\n",
    "print(f'Average sentiment score for the month: {np.mean(monthly_scores)}')\n",
    "for day, scores in day_scores.items():\n",
    "    print(f'Average sentiment score for {day}: {np.mean(scores)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18b7ce70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10', '11', '12', '13', '14', '17', '18', '19', '20', '21', '24', '25', '26', '27', '28', '3', '31', '4', '5', '6', '7']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "                                                  File Day  \\\n",
      "0    Apple_May_Not_Like_It_but_Zapple_Pay_Finds_Wor...  10   \n",
      "1    Bitcoin_Could_Rise_to_120K_by_End2024_Standard...  10   \n",
      "2    Bitcoin_Falls_Back_Below_31K_After_Late_Monday...  10   \n",
      "3    Bitcoin_podría_alcanzar_los_US120K_a_finales_d...  10   \n",
      "4    Bitcoin_Steady_Above_30K_as_China_Factory_Defl...  10   \n",
      "..                                                 ...  ..   \n",
      "130  Bitcoin_Retakes_30K_Asian_Stocks_Hit_5Week_Low...   7   \n",
      "131  BlackRock_CEOs_Turnabout_on_Bitcoin_Elicits_Ch...   7   \n",
      "132  First_Mover_Asia_Bitcoin_Whales_Are_Increasing...   7   \n",
      "133  Storj_Filecoin_and_Solana_Lead_First_Week_of_J...   7   \n",
      "134  US_Added_209K_Jobs_in_June_Missing_Expectation...   7   \n",
      "\n",
      "     Average Sentiment Score  \n",
      "0                   0.750995  \n",
      "1                  -0.271567  \n",
      "2                  -0.288226  \n",
      "3                   0.817198  \n",
      "4                  -0.054891  \n",
      "..                       ...  \n",
      "130                 0.127688  \n",
      "131                 0.753326  \n",
      "132                 0.389224  \n",
      "133                -0.482737  \n",
      "134                -0.028906  \n",
      "\n",
      "[135 rows x 3 columns]\n",
      "Average sentiment score for the month: 0.2252553105354309\n",
      "Average sentiment score for 10: 0.2508957087993622\n",
      "Average sentiment score for 11: 0.3977920114994049\n",
      "Average sentiment score for 12: -0.012107680551707745\n",
      "Average sentiment score for 13: 0.15092340111732483\n",
      "Average sentiment score for 14: 0.13420677185058594\n",
      "Average sentiment score for 17: 0.4053834080696106\n",
      "Average sentiment score for 18: 0.11432410776615143\n",
      "Average sentiment score for 19: 0.08993853628635406\n",
      "Average sentiment score for 20: 0.04032645747065544\n",
      "Average sentiment score for 21: 0.18643637001514435\n",
      "Average sentiment score for 24: 0.3448033332824707\n",
      "Average sentiment score for 25: 0.25051745772361755\n",
      "Average sentiment score for 26: 0.25512459874153137\n",
      "Average sentiment score for 27: 0.32734692096710205\n",
      "Average sentiment score for 28: 0.17571485042572021\n",
      "Average sentiment score for 3: 0.3427239656448364\n",
      "Average sentiment score for 31: 0.45570802688598633\n",
      "Average sentiment score for 4: 0.37972137331962585\n",
      "Average sentiment score for 5: 0.3850131034851074\n",
      "Average sentiment score for 6: 0.08924102783203125\n",
      "Average sentiment score for 7: 0.1517188549041748\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch.nn.functional as F  # Import the functional module from PyTorch\n",
    "from collections import defaultdict\n",
    "import numpy as np  #h\n",
    "# Load the FinBERT model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('ProsusAI/finbert')\n",
    "tokenizer = BertTokenizer.from_pretrained('ProsusAI/finbert')\n",
    "\n",
    "# Specify the directory containing the text files\n",
    "directory = r\"C:\\Users\\Rober\\Personal Projects\\STOCK MONEY\\Scrapers\\Data\\2023\\July\"\n",
    "\n",
    "def load_data(directory):\n",
    "    texts = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        print(dirs)\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                    texts.append((f.read(), os.path.basename(root), file))\n",
    "    return texts\n",
    "\n",
    "\n",
    "def segment_text(text, max_length=512, overlap=50):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    segments = []\n",
    "    segment = \"\"\n",
    "    for sentence in sentences:\n",
    "        if len(tokenizer.tokenize(segment + sentence)) <= max_length:\n",
    "            segment += \" \" + sentence\n",
    "        else:\n",
    "            segments.append(segment)\n",
    "            segment = sentence[max(0, len(sentence) - overlap):]\n",
    "    if segment:\n",
    "        segments.append(segment)\n",
    "    return segments\n",
    "\n",
    "def process_segments(segments):\n",
    "    # Tokenize all segments in a single batch, with padding enabled\n",
    "    inputs = tokenizer(\n",
    "        segments,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    return inputs['input_ids'], inputs['attention_mask']\n",
    "\n",
    "\n",
    "\n",
    "def get_predictions(model, dataloader):\n",
    "    model.eval()\n",
    "    sentiment_scores = []  # List to hold the sentiment scores\n",
    "    for batch in dataloader:\n",
    "        input_ids, attention_mask = [t.to(device) for t in batch]  # Move data to the device\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probs = F.softmax(logits, dim=1)  # Convert logits to probabilities\n",
    "        # Calculate sentiment score: prob(positive) - prob(negative)\n",
    "        scores = probs[:, 2] - probs[:, 0]\n",
    "        sentiment_scores.extend(scores.cpu())  # Move scores back to CPU for further processing\n",
    "    return sentiment_scores\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "texts = load_data(directory)\n",
    "\n",
    "# Initialize empty lists to hold file names, predicted labels, and dates\n",
    "all_files = []\n",
    "all_predicted_labels = []\n",
    "all_days = []\n",
    "day_scores = defaultdict(list)\n",
    "monthly_scores = []\n",
    "\n",
    "# Initialize Counters for daily and overall sentiment counts\n",
    "day_counts = {}\n",
    "overall_counts = Counter()\n",
    "\n",
    "label_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "\n",
    "# Process each file, generate predictions, and gather the results\n",
    "for text, day, file in texts:\n",
    "    segments = segment_text(text)\n",
    "    input_ids_tensor, attention_masks_tensor = process_segments(segments)\n",
    "    dataset = TensorDataset(input_ids_tensor, attention_masks_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=4)\n",
    "    sentiment_scores = get_predictions(model, dataloader)  # Get the sentiment scores\n",
    "    avg_sentiment_score = np.mean(sentiment_scores)  # Get the average sentiment score\n",
    "    day_scores[day].append(avg_sentiment_score)  # Store the average sentiment score\n",
    "    monthly_scores.append(avg_sentiment_score)\n",
    "    all_files.append(file)\n",
    "    all_days.append(day)\n",
    "\n",
    "# Create a DataFrame to hold the results\n",
    "results_df = pd.DataFrame({'File': all_files, 'Day': all_days, 'Average Sentiment Score': monthly_scores})\n",
    "\n",
    "# Optionally, save the results to a CSV file\n",
    "results_df.to_csv('classification_results.csv', index=False)\n",
    "\n",
    "# Print the results\n",
    "print(results_df)\n",
    "print(f'Average sentiment score for the month: {np.mean(monthly_scores)}')\n",
    "for day, scores in day_scores.items():\n",
    "    print(f'Average sentiment score for {day}: {np.mean(scores)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e421d771",
   "metadata": {},
   "source": [
    "# Classify 2022\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a11ee27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '10', '11', '12', '13', '14', '17', '18', '19', '20', '21', '22', '24', '25', '26', '27', '28', '29', '3', '4', '5', '6', '7', '8']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['1', '10', '11', '12', '14', '15', '16', '17', '18', '19', '2', '21', '22', '23', '24', '25', '26', '28', '29', '3', '30', '31', '4', '5', '7', '8', '9']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['1', '11', '12', '13', '14', '15', '16', '18', '19', '2', '20', '21', '22', '23', '25', '26', '27', '28', '29', '30', '4', '5', '6', '7', '8', '9']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['1', '10', '11', '13', '14', '15', '16', '17', '18', '2', '20', '21', '22', '23', '24', '25', '28', '3', '4', '7', '8', '9']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['10', '11', '12', '13', '14', '16', '18', '19', '20', '21', '24', '25', '26', '27', '28', '29', '3', '31', '4', '5', '6', '7', '8', '9']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['1', '10', '11', '12', '13', '14', '15', '17', '18', '19', '2', '20', '21', '22', '24', '25', '26', '27', '28', '29', '3', '31', '4', '5', '6', '7', '8']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['1', '10', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '26', '27', '28', '29', '3', '30', '5', '6', '7', '8', '9']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['1', '10', '11', '13', '14', '15', '16', '17', '18', '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '30', '31', '4', '6', '7', '8', '9']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '22', '23', '24', '25', '26', '27', '29', '3', '30', '31', '4', '5', '6', '8', '9']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['1', '10', '11', '14', '15', '16', '17', '18', '19', '2', '21', '22', '23', '24', '25', '27', '28', '29', '3', '30', '4', '6', '7', '8', '9']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['10', '11', '12', '13', '14', '16', '17', '18', '19', '2', '20', '21', '24', '25', '26', '27', '28', '3', '30', '31', '4', '5', '6', '7', '9']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['1', '11', '12', '13', '14', '15', '16', '18', '19', '2', '20', '21', '22', '23', '26', '27', '28', '29', '30', '4', '5', '6', '7', '8', '9']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch.nn.functional as F  # Import the functional module from PyTorch\n",
    "from collections import defaultdict\n",
    "import numpy as np  #h\n",
    "# Load the FinBERT model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('ProsusAI/finbert')\n",
    "tokenizer = BertTokenizer.from_pretrained('ProsusAI/finbert')\n",
    "# Specify the directory containing the text files\n",
    "directory = r\"C:\\Users\\Rober\\Personal Projects\\STOCK MONEY\\Scrapers\\Data\\2022\"\n",
    "\n",
    "\n",
    "def load_data(directory):\n",
    "    texts = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                    texts.append((f.read(), os.path.basename(root), file))\n",
    "    return texts\n",
    "\n",
    "\n",
    "def segment_text(text, max_length=512, overlap=50):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    segments = []\n",
    "    segment = \"\"\n",
    "    for sentence in sentences:\n",
    "        if len(tokenizer.tokenize(segment + sentence)) <= max_length:\n",
    "            segment += \" \" + sentence\n",
    "        else:\n",
    "            segments.append(segment)\n",
    "            segment = sentence[max(0, len(sentence) - overlap):]\n",
    "    if segment:\n",
    "        segments.append(segment)\n",
    "    return segments\n",
    "\n",
    "def process_segments(segments):\n",
    "    # Tokenize all segments in a single batch, with padding enabled\n",
    "    inputs = tokenizer(\n",
    "        segments,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    return inputs['input_ids'], inputs['attention_mask']\n",
    "\n",
    "def process_month(directory):\n",
    "    texts = load_data(directory)\n",
    "    all_files = []\n",
    "    all_predicted_labels = []\n",
    "    all_days = []\n",
    "    day_scores = defaultdict(list)\n",
    "    monthly_scores = []\n",
    "    \n",
    "    # ... (rest of your processing logic)\n",
    "    \n",
    "    # Create a DataFrame to hold the results\n",
    "    results_df = pd.DataFrame({'File': all_files, 'Day': all_days, 'Average Sentiment Score': monthly_scores})\n",
    "    \n",
    "    # Save the results to a CSV file named after the month\n",
    "    month_name = os.path.basename(directory)\n",
    "    results_df.to_csv(f'classification_results_{month_name}.csv', index=False)\n",
    "\n",
    "def get_predictions(model, dataloader):\n",
    "    model.eval()\n",
    "    sentiment_scores = []  # List to hold the sentiment scores\n",
    "    for batch in dataloader:\n",
    "        input_ids, attention_mask = [t.to(device) for t in batch]  # Move data to the device\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probs = F.softmax(logits, dim=1)  # Convert logits to probabilities\n",
    "        # Calculate sentiment score: prob(positive) - prob(negative)\n",
    "        scores = probs[:, 2] - probs[:, 0]\n",
    "        sentiment_scores.extend(scores.cpu())  # Move scores back to CPU for further processing\n",
    "    return sentiment_scores\n",
    "def process_texts(texts):\n",
    "    all_files = []\n",
    "    all_days = []\n",
    "    day_scores = defaultdict(list)\n",
    "    monthly_scores = []\n",
    "\n",
    "    for text, day, file in texts:\n",
    "        segments = segment_text(text)\n",
    "        input_ids_tensor, attention_masks_tensor = process_segments(segments)\n",
    "        dataset = TensorDataset(input_ids_tensor, attention_masks_tensor)\n",
    "        dataloader = DataLoader(dataset, batch_size=4)\n",
    "        sentiment_scores = get_predictions(model, dataloader)\n",
    "        avg_sentiment_score = np.mean(sentiment_scores)\n",
    "        day_scores[day].append(avg_sentiment_score)\n",
    "        monthly_scores.append(avg_sentiment_score)\n",
    "        all_files.append(file)\n",
    "        all_days.append(day)\n",
    "        \n",
    "    return all_files, all_days, monthly_scores, day_scores\n",
    "\n",
    "# Function to process each month\n",
    "def process_month(directory):\n",
    "    texts = load_data(directory)\n",
    "    all_files, all_days, monthly_scores, day_scores = process_texts(texts)\n",
    "    \n",
    "    # Create the new directory if it doesn't exist\n",
    "    new_dir = os.path.join(directory, '2022_classification')\n",
    "    os.makedirs(new_dir, exist_ok=True)  # 'exist_ok=True' will prevent an error if the directory already exists\n",
    "    \n",
    "    # Adjust the file path to include the new directory\n",
    "    month_name = os.path.basename(directory)\n",
    "    results_file_path = os.path.join(new_dir, f'classification_results_{month_name}.csv')\n",
    "    results_df = pd.DataFrame({'File': all_files, 'Day': all_days, 'Average Sentiment Score': monthly_scores})\n",
    "    results_df.to_csv(results_file_path, index=False)\n",
    "\n",
    "\n",
    "# Main loop to process each month\n",
    "def main(directory):\n",
    "    months = [os.path.join(directory, month_dir) for month_dir in os.listdir(directory) if os.path.isdir(os.path.join(directory, month_dir))]\n",
    "    for month_directory in months:\n",
    "        process_month(month_directory)\n",
    "\n",
    "# Set device for PyTorch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Call main function to process each month\n",
    "main(directory)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
